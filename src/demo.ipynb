{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anup/anaconda3/envs/venv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import falconn\n",
    "import time\n",
    "from natsort import natsorted\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import Audio, AudioFeature, Augmentations, Array\n",
    "from train import ContrastiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbase = pickle.load(open(\"/home/anup/PB_imp_files/aa/EMB_DB.pkl\", \"rb\"))\n",
    "# dbase = dbase.getdata()\n",
    "# # dbase.dtype\n",
    "# np.argwhere(np.isnan(dbase))\n",
    "# dbase1 = np.delete(dbase, 375467, axis=0)\n",
    "# np.argwhere(np.isnan(dbase1))\n",
    "# np.save(\"EMB_DB.npy\", dbase1.astype('float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anup/anaconda3/envs/venv/lib/python3.7/site-packages/numpy/linalg/linalg.py:2561: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3.17e+02, 5.89e+01],\n",
       "        [3.17e+02, 5.90e+01],\n",
       "        [3.18e+02, 0.00e+00],\n",
       "        [3.18e+02, 1.00e-01]], dtype=float16),\n",
       " '/home/anup/PARAMSANAGANK_backup/data/PB/train/C1/41457_3.wav',\n",
       " array([254.4,   inf, 246. , 241.5], dtype=float16),\n",
       " array([1., 0., 1., 1.], dtype=float16))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = pickle.load(open(\"/home/anup/PB_imp_files/aa/FILES_C.pkl\", \"rb\"))\n",
    "meta = pickle.load(open(\"/home/anup/PB_imp_files/aa/METADATA_C.pkl\", \"rb\"))\n",
    "db = pickle.load(open(\"/home/anup/PB_imp_files/aa/EMB_DB_C.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "# files.getdata()[300:350], \n",
    "meta.getdata()[184696:184700], files.getdata()[318], np.linalg.norm(db.getdata()[184696:184700], axis=1), np.linalg.norm(db.getdata()[184696:184700]/np.linalg.norm(db.getdata()[184696:184700], axis=1).reshape(-1,1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH_Index():\n",
    "    \"\"\"LSH Indexer\"\"\"\n",
    "    def __init__(self, dbase, tables=50, bits=18, probes=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            dbase: (np.ndarray), reference database of fingerprints\n",
    "            tables: (int, optional), LSH parameter, no. of hash tables to create\n",
    "            bits: (int, optional), no. of bits to encode fingeprints\n",
    "            probes: (int, optional), LSH parameter, total no. of hash buckets to probe across hash tables\n",
    "        \"\"\"\n",
    "        self.dbase = dbase\n",
    "        self.tables = tables\n",
    "        self.bits = bits\n",
    "        self.probes = probes\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build index structure using LSH\"\"\"\n",
    "        self.dbase = self.dbase/np.linalg.norm(self.dbase, axis=1).reshape(-1,1)\n",
    "        # center = np.mean(self.dbase, axis=0)\n",
    "        # self.dbase -= center\n",
    "        # self.dbase = self.dbase[:1000]\n",
    "\n",
    "        print(\"Indexing database...\")\n",
    "        number_of_tables = self.tables\n",
    "        params_cp = falconn.LSHConstructionParameters()\n",
    "        params_cp.dimension = len(self.dbase[1])\n",
    "        params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
    "        params_cp.distance_function = falconn.DistanceFunction.NegativeInnerProduct\n",
    "        params_cp.l = number_of_tables\n",
    "        params_cp.num_rotations = 1\n",
    "        params_cp.seed = 5721840\n",
    "        params_cp.num_setup_threads = 0\n",
    "        params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
    "        falconn.compute_number_of_hash_functions(self.bits, params_cp)\n",
    "\n",
    "        table = falconn.LSHIndex(params_cp)\n",
    "        table.setup(self.dbase)\n",
    "        \n",
    "        query_object = table.construct_query_object()\n",
    "        query_object.set_num_probes(self.probes)\n",
    "        \n",
    "        return query_object\n",
    "\n",
    "\n",
    "class Search():\n",
    "    \"\"\"\n",
    "    LSH-based indexer to perform audio retrieval\n",
    "    \"\"\"\n",
    "    def __init__(self,checkpoint, metadata, query_object, seglen=960, fs=16000, featparams={\"n_fft\":512, \"hop_length\":160, \"n_mels\":64}, mode=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ---------\n",
    "            checkpoint: (str), model weights path\n",
    "            metadata: (list), [METADATA, FILENAMES], METADATA looks like: [[file ID_1, timestamp_1], [file ID_1, timestamp_2], ...[file ID_A, timestamp_N]]. FILENAMES looks like [FILENAME_1, ..., FILENAME_A]\n",
    "            query_object: (class object), LSH class instance\n",
    "            seglen: (int, optional), fingerprint length in ms. This is fixed, it cannot be changed.\n",
    "            fs: (int, optional), sampling rate of an audio\n",
    "            featparams: (dict, optional), required parameters to transform signal to log Mel spectrogram\n",
    "            mode: (str, optional), perform search on either \"cpu\" or \"cuda\" device.\n",
    "        \"\"\"    \n",
    "        self.dbase_meta = pickle.load(open(metadata[0], \"rb\")).getdata()\n",
    "        self.files = pickle.load(open(metadata[1], \"rb\")).getdata()\n",
    "        self.query_object = query_object\n",
    "        self.seglen = seglen\n",
    "        self.fs =fs  \n",
    "        self.featextract = AudioFeature(n_fft=featparams['n_fft'], hop_length=featparams['hop_length'], n_mels=featparams['n_mels'], fs=self.fs) #STFT parameters\n",
    "        self.extractor = self.featextract.get_log_mel_spectrogram \n",
    "        self.audioreader = Audio()\n",
    "        self.mode= mode\n",
    "        print(\"Loading fingerprinter...\")\n",
    "        self.model = ContrastiveModel.load_from_checkpoint(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(torch.device(self.mode))\n",
    "\n",
    "\n",
    "    def preprocess_signal(self,filepath):\n",
    "        \"\"\"\n",
    "        Reads audio signal stored at <filepath>\n",
    "        Parameters:\n",
    "        ----------\n",
    "            filepath: (str), file path of an audio file\n",
    "\n",
    "        Returns: \n",
    "        -------\n",
    "            audio: (float32 tensor), preprocessed audio data\n",
    "        \"\"\"\n",
    "        audio = self.audioreader.read(filepath)\n",
    "        return audio\n",
    "    \n",
    "    def get_segments(self, audio, hop=100):\n",
    "        \"\"\"Generates consecutive segments of length 1s in an <audio> track with a hop rate of <hop> samples. \n",
    "        Parameters:\n",
    "        -----------\n",
    "            audio: (float32 tensor), clean audio signal\n",
    "            hop: (int, optional), window hop length in samples. default=100 (0.1 s)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            chunks: (float32 2D tensor), Batch of spectrograms corresponding to segments of fixed length\n",
    "        \"\"\"\n",
    "        hop = int(hop*0.1) # hop in no. of frames in spectrogram \n",
    "        seglen = int(self.seglen*0.1) # segment length in terms of no. of frames in spectrogram. 0.96s means 96 frames\n",
    "\n",
    "        spectrum = self.extractor(audio)[:, :-1]\n",
    "        chunks = [spectrum[:,i:i+seglen] for i in range(0,spectrum.shape[1]-seglen-1, hop)]\n",
    "        chunks = torch.stack(chunks).unsqueeze(1)\n",
    "        return  chunks\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_embeddings(self, chunks):\n",
    "        \"\"\"Generates embeddings for a batch(size N) of segments generated from an audio file\n",
    "        Parameters:\n",
    "        ----------\n",
    "            chunks: (float32 tensor), Batch(size N) of spectrograms corresponding to segments of fixed length. \n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            fp: (np.ndarray, float32), sub fingerprints. Dims: N x emb_dim\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self.mode == \"cuda\":\n",
    "                fp = self.model.predict_step(chunks.to(torch.device(\"cuda\")), 1)    \n",
    "            else:\n",
    "                fp = self.model.predict_step(chunks, 1)   \n",
    "        return fp\n",
    "\n",
    "    # find nearest neighbors for each subfingerprint using LSH\n",
    "    def lookup(self, queries):\n",
    "        \"\"\"\n",
    "        Performs audio retrieval process\n",
    "        Parameters:\n",
    "        ----------\n",
    "            queries: (float32 tensor), batch of embeddings of size NxD (no of subfp x fp dims)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            id_match: (int), matched fileID index \n",
    "            timeoffset: (float), located query timestamp in identified audio file\n",
    "            l_evidence: (int), number of subfingerprints supporting computed timeoffset \n",
    "            cands: (int), average number of search candidates for each sub-fingerprint \n",
    "        \"\"\"\n",
    "        \n",
    "        emb_idx = []\n",
    "        cands = 0\n",
    "\n",
    "        if len(queries.shape) == 1:\n",
    "            queries = queries.reshape(1,-1)\n",
    "\n",
    "        for idx in range(len(queries)):\n",
    "            i = self.query_object.find_k_nearest_neighbors(queries[idx], 5) # search top-5 matches for each subfingerprint\n",
    "            cands += len(self.query_object.get_unique_candidates(queries[idx]))\n",
    "            emb_idx.append(i)\n",
    "\n",
    "        cands = cands/len(queries)\n",
    "        emb_idx = np.asarray(emb_idx, dtype=int)\n",
    "\n",
    "        # identify the audio file and keep only mathches that comes from it\n",
    "        topk=5 \n",
    "        ele,c = np.unique(self.dbase_meta[emb_idx[:,:topk]][:,:,0], return_counts=True)\n",
    "        top_file = ele[np.argmax(c)]\n",
    "        # id_match = np.sum(self.dbase_meta[emb_idx[:,:topk]][:,0,0]== top_file)\n",
    "\n",
    "        mask = self.dbase_meta[emb_idx[:,:topk]][:,:,0] == top_file\n",
    "        emb_idx[~mask] = -1\n",
    "        A = []\n",
    "        rank = 0\n",
    "        for i in range(len(emb_idx[:,rank])):\n",
    "            if emb_idx[:,rank][i] >= 0:\n",
    "                A.append(np.arange(emb_idx[:,rank][i]-i,emb_idx[:,rank][i]-i+len(emb_idx[:,rank])) - emb_idx[:,0])\n",
    "        A = np.array(A)\n",
    "        if len(A) == 0:\n",
    "            return \"-1\",-1,-1,-1\n",
    "        else:\n",
    "            U, C = np.unique(np.asarray(A, dtype=int), axis=0, return_counts=True) # get all unique sequence candidates and their counts\n",
    "            evidence = np.where((A == U[np.argmax(np.sum(U==0,axis=1))]).all(axis=1))[0]  # sequence cand with max counts\n",
    "            offset =self.dbase_meta[(A[evidence]+emb_idx[:,0])[0,0]][1] # time stamp of the first index of sequence candidate\n",
    "            return self.files[int(top_file)-1], offset, len(evidence), cands\n",
    "    \n",
    "    def subfingerprints_search(self,query):\n",
    "        \"\"\"Identifies reference audio and locate matching timestamp\n",
    "        Parameters:\n",
    "        ----------\n",
    "            query: (str or tensor), query\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            id_match: (int), matched fileID index \n",
    "            timeoffset: (float), located query timestamp in identified audio file\n",
    "            l_evidence: (int), number of subfingerprints supporting computed timeoffset \n",
    "            cands: (int), average number of search candidates for each sub-fingerprint \n",
    "        \"\"\"\n",
    "        if isinstance(query, str):\n",
    "            audio_positive = self.preprocess_signal(query)\n",
    "        else: \n",
    "            audio_positive = query\n",
    "\n",
    "        chunks= self.get_segments(audio_positive)\n",
    "\n",
    "        # generate embeddings/subfingerprints\n",
    "        fps = self.generate_embeddings(chunks)\n",
    "        queries = F.normalize(fps, dim=-1)\n",
    "        queries = queries.cpu().numpy()\n",
    "        id_match, timeoffset, l_evidence,cands = self.lookup(queries)\n",
    "        \n",
    "        return id_match, timeoffset, l_evidence, cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing database...\n",
      "Loading fingerprinter...\n"
     ]
    }
   ],
   "source": [
    "with open(\"/scratch/sanup/PB_optimized/config/search.yaml\", 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Paths to reference database and trained model\n",
    "dbase_meta = [cfg[\"metadata\"], cfg[\"files\"]]\n",
    "dbase = pickle.load(open(cfg[\"emb_db\"], \"rb\"))\n",
    "dbase = dbase.getdata()\n",
    "\n",
    "indexer = LSH_Index(dbase, tables=50, bits=18, probes=1000)\n",
    "query_object = indexer.build_index()\n",
    "\n",
    "# Path containing list of database audio filenames\n",
    "ckpt_path= cfg[\"checkpoint\"]\n",
    "API = Search(ckpt_path, dbase_meta, query_object, mode=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.569625 /scratch/sanup/data/PB/train/B/41588_10.wav\n",
      "/scratch/sanup/data/PB/train/B/41588_10.wav 47.6 38 178241.07317073172 3.7048332691192627\n",
      "25.82075 /scratch/sanup/data/PB/train/B/37660_1.wav\n",
      "/scratch/sanup/data/PB/train/B/37660_1.wav 25.8 38 108881.9512195122 2.9457898139953613\n",
      "25.163625 /scratch/sanup/data/PB/train/B/45544_13.wav\n",
      "/scratch/sanup/data/PB/train/B/45544_13.wav 25.2 39 206650.68292682926 4.157587051391602\n",
      "36.2409375 /scratch/sanup/data/PB/train/B/24347_1.wav\n",
      "/scratch/sanup/data/PB/train/B/24347_1.wav 36.2 23 175284.70731707316 3.590177536010742\n",
      "29.329625 /scratch/sanup/data/PB/train/B/24966_1.wav\n",
      "/scratch/sanup/data/PB/train/B/24966_1.wav 29.3 31 129061.51219512195 3.086439609527588\n",
      "1.1910625 /scratch/sanup/data/PB/train/B/45546_19.wav\n",
      "/scratch/sanup/data/PB/train/B/45546_11.wav 49.0 2 156405.8780487805 4.126189231872559\n",
      "23.0888125 /scratch/sanup/data/PB/train/B/45886_3.wav\n",
      "/scratch/sanup/data/PB/train/B/45886_3.wav 23.1 37 143985.78048780488 3.249454975128174\n",
      "25.4814375 /scratch/sanup/data/PB/train/B/25080_5.wav\n",
      "/scratch/sanup/data/PB/train/B/25080_5.wav 25.5 41 124139.78048780488 3.019684314727783\n",
      "3.8429375 /scratch/sanup/data/PB/train/B/45527_31.wav\n",
      "-1 -1 -1 -1 4.056567668914795\n",
      "2.330375 /scratch/sanup/data/PB/train/B/24324_1.wav\n",
      "/scratch/sanup/data/PB/train/B/24324_1.wav 2.3 41 174425.73170731709 3.3655383586883545\n"
     ]
    }
   ],
   "source": [
    "fs = 16000\n",
    "files = natsorted(glob.glob(\"/scratch/sanup/data/PB/train/B/**/*.wav\", recursive=True))\n",
    "# files = np.random.choice(files, 1000, replace=False)\n",
    "noises = natsorted(glob.glob(\"/scratch/sanup/data/distortions/RIRS_NOISES/pointsource_noises/*.wav\"))\n",
    "rirs = natsorted(glob.glob(\"/scratch/sanup/data/distortions/RIRS_NOISES/real_rirs_isotropic_noises/*.wav\"))\n",
    "\n",
    "reader = Audio()\n",
    "distorter = Augmentations()\n",
    "feat_extractor = AudioFeature(n_fft=512,hop_length=160, n_mels=64, fs=fs)\n",
    "\n",
    "\n",
    "rir04 = reader.read(rirs[2])\n",
    "rir05 = reader.read(rirs[3])\n",
    "snr=0\n",
    "length=5\n",
    "for i in range(10):\n",
    "    query_fname = np.random.choice(files, 1)[0]\n",
    "    query_audio = reader.read(query_fname)\n",
    "    offset_with_buffer = np.random.randint(len(query_audio) - (fs*(length+1)) - 1)\n",
    "    noise = reader.read(np.random.choice(noises))\n",
    "    \n",
    "    #create noise and noise+reverb added query\n",
    "    noise_query = distorter.add_noise(query_audio[offset_with_buffer+fs: offset_with_buffer+fs+(fs*length)], noise, snr)\n",
    "    noise_reverb_04_query = distorter.add_noise_reverb(query_audio[offset_with_buffer:offset_with_buffer+(1+length)*fs], noise, snr, rir04)[fs: (1+length)*fs]\n",
    "    noise_reverb_05_query = distorter.add_noise_reverb(query_audio[offset_with_buffer:offset_with_buffer+(1+length)*fs], noise, snr, rir05)[fs: (1+length)*fs]\n",
    "\n",
    "    query_timeoffset = str((offset_with_buffer + fs)/fs)\n",
    "    # filename = query_timeoffset+\"_\"+query_fname.split(\"/\")[-2] + \"_\" +query_fname.split(\"/\")[-1].split('.')[0]\n",
    "    print(query_timeoffset, query_fname)\n",
    "    s = time.time()\n",
    "    songid, timeoffset, levi, cands  = API.subfingerprints_search(noise_query)\n",
    "    print(songid, timeoffset, levi, cands, time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2959463258d0367096de30e1d8dc5408fd5a58f873befec6b27db5c66266ec77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
